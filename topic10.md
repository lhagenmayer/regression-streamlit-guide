# Machine Learning 1 - AusfÃ¼hrliches Skript
## Fundamentals: Motivation, Workflow, KNN und Regression

---

## ğŸ“‹ Inhaltsverzeichnis

1. [Motivation fÃ¼r KÃ¼nstliche Intelligenz](#1-motivation-fÃ¼r-kÃ¼nstliche-intelligenz)
2. [Grundkonzepte des Machine Learning](#2-grundkonzepte-des-machine-learning)
3. [Historischer Abriss der KI](#3-historischer-abriss-der-ki)
4. [Machine Learning Lerntypen](#4-machine-learning-lerntypen)
5. [Der ML-Workflow](#5-der-ml-workflow)
6. [Daten: Kernkomponente jedes Modells](#6-daten-kernkomponente-jedes-modells)
7. [Train/Validation/Test Split](#7-trainvalidationtest-split)
8. [K-Fold Cross-Validation](#8-k-fold-cross-validation)
9. [Datenexploration und Feature Engineering](#9-datenexploration-und-feature-engineering)
10. [K-Nearest Neighbors (KNN)
Klassifizierung](#10-k-nearest-neighbors-knn-klassifizierung)
11. [Daten-Skalierung](#11-daten-skalierung)
12. [Lineare Regression](#12-lineare-regression)
13. [Logistische Regression und
Entscheidungsgrenzen](#13-logistische-regression-und-entscheidungsgrenzen)

---

## 1. Motivation fÃ¼r KÃ¼nstliche Intelligenz

### 1.1 Warum brauchen wir AI/ML?

**Kernidee:** Machine Learning hilft uns, **praktische Probleme zu lÃ¶sen**, die zu komplex
sind, um sie explizit zu programmieren.

**Beispiel 1: Turbinen-Vorhersage**
- **Problem:** Turbinen in Wasserkraftwerken fallen nach 5-10 Jahren aus
- **Warum lÃ¶sen?** Der ungeplante Austausch ist sehr teuer
- **ML-LÃ¶sung:** Modell trainieren, das vorhersagt, **wann** ein Ausfall eintritt
- **Ergebnis:** Geplanter Austausch statt Notfall

**Beispiel 2: Schnelle medizinische Diagnose**
- **Problem:** Troponin-Messung im Blut dauert etwa 1 Stunde
- **Warum lÃ¶sen?** Schnelle Erkennung kÃ¶nnte Leben retten
- **ML-LÃ¶sung:** Modell trainieren, das die Troponin-Konzentration vorhersagt
- **Ergebnis:** Schnellere Diagnose mÃ¶glich

### 1.2 Das Grundprinzip: Lernen aus Mustern

**Definition:** AI/ML funktioniert immer nach demselben Prinzip:
1. Vorhandene Muster in historischen Daten **erkennen**
2. Diese Muster auf **neue, Ã¤hnliche Situationen** Ã¼bertragen
3. **Vorhersagen** fÃ¼r die Zukunft treffen

**Metapher:** Ein Arzt sieht viele Patienten mit Ã¤hnlichen Symptomen,
lernt Muster (dieses Symptom bedeutet meist X), und kann dann bei neuen Patienten besser
diagnostizieren.

### 1.3 Ausgabeformate von AI-Modellen

Ein trainiertes Modell kann verschiedene Ausgaben produzieren:

**Qualitative Ausgabe (Kategorie):**
- *Input:* Ein Foto
- *Output:* "Dieser Hund ist mit 95% Wahrscheinlichkeit ein Bernhardiner"

**Quantitative Ausgabe (Zahl):**
- *Input:* Alter 65, Blutzucker 160 mg/dL
- *Output:* "90% Wahrscheinlichkeit fÃ¼r Diabetes"

### 1.4 Essenzielle Voraussetzungen

**Ohne Daten kein Modell!** Das ist absolute Voraussetzung.

**Warum?** Auch wenn wir ein Modell bauen kÃ¶nnen, ohne dass es programmiert ist:
- Das Modell **trainieren** wir, statt es zu programmieren
- Ein Rezept benÃ¶tigt Zutaten und deren VerhÃ¤ltnisse
- Ein ML-Modell benÃ¶tigt Beispieldaten

**Wichtig:** Die Trainingsdaten mÃ¼ssen **Ã¤hnlich** zu den Daten sein, auf die das Modell
spÃ¤ter angewendet wird.

**Gegenbeispiel:** Ein Bildklassifizierer trainiert auf SchweiÃŸnÃ¤hte in speziellen
Industriekameras funktioniert nicht fÃ¼r KreditbonitÃ¤tsbewertung.

---

## 2. Grundkonzepte des Machine Learning

### 2.1 Was ist Machine Learning genau?

**Definition (Arthur Samuel, 1959):**
> "The field of study that gives computers the ability to learn without being explicitly
programmed"

**Praktisch bedeutet das:**
- Computer lernen von Beispielen (Daten)
- Statt festcodierte Regeln â†’ Der Algorithmus findet Regeln
- Nach ausreichend vielen Beispielen: Generalisierung auf neue FÃ¤lle

**Beispiel - Bildklassifizierung:**
- Wir zeigen dem Modell 10.000 markierte Bilder (Hund ja/nein)
- Das Modell lernt: "Welche Merkmale = Hund?"
- Bei neuen Bildern: Anwendung der gelernten Regeln

**Beispiel - Textgenerierung:**
- Modell liest 5 Millionen BÃ¼cher
- Lernt: "Nach dem Wort 'Guten' kommt hÃ¤ufig 'Morgen'"
- Kann jetzt SÃ¤tze fortsetzen: "Der Himmel ist..." â†’ "blau"

### 2.2 Machine Learning vs. Statistik

Machine Learning **baut auf** statistischen Methoden auf, erweitert sie aber:

| Aspekt | Statistik | Machine Learning |
|--------|-----------|------------------|
| Fokus | Inferenz (RÃ¼ckschlÃ¼sse) | Vorhersage |
| Ziel | p-Werte, Konfidenzintervalle | Genauigkeit auf neuen Daten |
| Fragestellung | Ist dieser Effekt signifikant? | Kann ich Zukunft vorhersagen? |

### 2.3 Disziplinen die ML bilden

Machine Learning ist ein **Schnittfeld** mehrerer Disziplinen:

```
Statistik + Mathematik + Informatik = Machine Learning
     +                      +
  Ã–konomie/Psychologie      GPU-Computing
     +                      +
  Neurowissenschaften    Software Engineering
```

**Statistik:** RÃ¼ckschlÃ¼sse aus Daten, Unsicherheitsquantifizierung
**Mathematik:** Optimierungsalgorithmen, Lineare Algebra
**Informatik:** Datenstrukturen, Algorithmen, groÃŸe Datenmengen
**Ã–konomie:** Wie optimiert ein System seine Performance?
**Neurowissenschaften:** Inspiration fÃ¼r Neural Networks

---

## 3. Historischer Abriss der KI

### 3.1 Zeitstrahl der KI-Entwicklung

#### 1941-1950er: AnfÃ¤nge
- **1941:** Z3 - erster digitaler Computer
- **1950:** Alan Turing's Frage: "Can machines think?"
- **Grundlagen:** Logik + Informationstheorie = "Cybernetics"

#### 1956: Dartmouth Workshop - Geburt der KI
- 6-wÃ¶chiger Workshop mit fÃ¼hrenden Forschern
  - Marvin Minsky
  - John McCarthy
  - Claude Shannon
  - â€¦und andere
- **Erste Zieldefinition der KI:**
> "Schreibe ein Programm, das intellektuelle Probleme lÃ¶sen kann, ebenso gut oder besser
als ein Mensch"

#### 1958: Das Perceptron
- Erste praktische Implementierung eines neuronalen Netzwerks
- **Physisches GerÃ¤t** mit manuell einstellbaren Gewichten
- Konnte viele Probleme lÃ¶sen â†’ GroÃŸe Hoffnungen!

#### 1969: AI Winter Beginn
- **Buch:** "Perceptrons" (Minsky & Papert)
- **Erkenntnis:** ANNs kÃ¶nnen das XOR-Problem nicht lÃ¶sen
- **Folge:** FÃ¶rdermittel versiegen, Forschung stagniert
- **Problem (Moravecs Paradoxon):** 
  - ANNs kÃ¶nnen explizite Probleme lÃ¶sen
  - Aber: Sie kÃ¶nnen nicht **sehen** oder **hÃ¶ren**

#### 1980er: Expert Systems und 2. AI Winter
- **Kurzfristige Renaissance:** Expertensysteme mit vorprogrammierten Regeln
- **Problem:** Limited applicability, keine LernfÃ¤higkeit
- **Folge:** Wieder Stagnation

#### 1986: Backpropagation "wiederentdeckt"
- Rumelhart et al. zeigen: **ANNs kÃ¶nnen trainiert werden!**
- Neue Methode: Backpropagation (Gradient Descent)
- Langsamer Aufschwung beginnt

#### 1990er-2000er: Langsame Fortschritte
- **DeepBlue** (1997): Schach-Engine besiegt Gary Kasparov
- **Watson** (2011): Gewinnt Jeopardy gegen Champions
- Beginn: GPU-Computing macht alles schneller

#### 2012: AlexNet - Das Breakthrough
- **Convolutional Neural Network** (CNN) auf **2 GPUs**
- ImageNet Challenge: **Dramatischer Sprung** in Genauigkeit
- Ursachen:
  1. GPUs machen Training effizient
  2. Massive Datenmengen verfÃ¼gbar (Internet)
  
#### 2010er: Deep Learning Dominanz
- Tiefe neuronale Netzwerke Ã¼bertreffen fast alle anderen ML-Methoden
- Zwei kritische Faktoren:
  1. **GPU-Training:** 10-100x schneller
  2. **DatenverfÃ¼gbarkeit:** Big Data Ã¼berall

#### 2017: Transformer-Architektur
- **Paper:** "Attention is all you need" (Vaswani et al.)
- **Auswirkung:** Grundlage fÃ¼r GPT, BERT, groÃŸe Sprachmodelle
- **Aktuelle Ã„ra:** Generative AI beginnt

### Neural Network Architecture

```mermaid
graph TD
    A[" Input Layer<br/>Rohdaten"] --> B[" Hidden Layer 1<br/>Feature Extraction"]
    B --> C[" Hidden Layer 2<br/>Pattern Recognition"]
    C --> D[" Output Layer<br/>Vorhersage"]

    A --> A1[Features: xâ‚, xâ‚‚, ..., xâ‚™]
    B --> B1[Neurons mit Gewichten]
    B --> B2[Activation Functions]
    C --> C1[Komplexere Patterns]
    D --> D1[Classification Scores]

    E[Forward Propagation] --> E1[Input â†’ Hidden â†’ Output]
    E --> E2[Gewichtete Summe + Bias]
    E --> E3[Activation Function]

    F[Backpropagation] --> F1[Fehler berechnen]
    F --> F2[Gradient descent]
    F --> F3[Gewichte aktualisieren]

    G[Convolutional NN] --> G1[Conv Layers fÃ¼r Bilder]
    G --> G2[Pooling fÃ¼r Feature Maps]
    G --> G3[Fully Connected am Ende]

    H[Recurrent NN] --> H1[LSTM/GRU fÃ¼r Sequenzen]
    H --> H2[Memory fÃ¼r zeitliche AbhÃ¤ngigkeiten]
    H --> H3[NatÃ¼rliche Sprache, Zeitserien]

    style A fill:#e8f5e8
    style D fill:#c8e6c9
```

---

## 4. Machine Learning Lerntypen

### 4.1 Systematische Kategorisierung

ML wird nach **zwei Dimensionen** kategorisiert:

#### Dimension 1: ZielgrÃ¶ÃŸe-Typ

**Kontinuierliche ZielgrÃ¶ÃŸe** (Regression):
- Output: Beliebige Zahlenwerte
- Beispiele: Preis, Temperatur, Wahrscheinlichkeit
- Formel: $y \in \mathbb{R}$ (alle reellen Zahlen)

**Kategorische ZielgrÃ¶ÃŸe** (Klassifizierung):
- Output: Diskrete Kategorien/Klassen
- Beispiele: Hund/Katze, Spam/Kein Spam, Obstsorte
- Formel: $y \in \{C_1, C_2, \ldots, C_n\}$

#### Dimension 2: VerfÃ¼gbarkeit von Labels

**Ãœberwachtes Lernen (Supervised):**
- Trainigsdaten haben **Labels** (y-Werte)
- Beispiel: 1000 Bilder mit Markierung "Katze" oder "Nicht-Katze"
- Unser Fokus in diesem Kurs

**UnÃ¼berwachtes Lernen (Unsupervised):**
- Trainigsdaten haben **keine Labels**
- Ziel: Struktur in Daten finden
- Beispiele:
  - **Clustering:** Ã„hnliche Kunden gruppieren
  - **Outlier Detection:** Anomalien in Serverlogs finden

**Reinforcement Learning (mit Feedback):**
- Agent lernt durch Trial-and-Error
- Feedback: "Belohnung" oder "Bestrafung"
- Beispiel: AlphaGo lernt Go durch Millionen Spiele

### 4.2 ML-Typen Matrix

```mermaid
graph TD
    A[" Machine Learning"] --> B[" Supervised Learning"]
    A --> C[" Unsupervised Learning"]
    A --> D[" Reinforcement Learning"]

    B --> B1[Regression<br/>Kontinuierliche Vorhersage]
    B --> B2[Classification<br/>Kategorische Vorhersage]

    C --> C1[Clustering<br/>Muster finden]
    C --> C2[Dimension Reduction<br/>Struktur verstehen]

    D --> D1[Adaptive Systems<br/>Trial-and-Error Learning]

    B1 --> E[Lineare Regression]
    B1 --> F[Polynomiale Regression]

    B2 --> G[Logistic Regression]
    B2 --> H[Decision Trees]
    B2 --> I[Support Vector Machines]

    C1 --> J[K-Means]
    C1 --> K[Hierarchical Clustering]

    D1 --> L[AlphaGo]
    D1 --> M[Roboter Control]
```

| Aufgabe | Kontinuierliche ZielgrÃ¶ÃŸe | Kategorische ZielgrÃ¶ÃŸe |
|---------|---------------------------|-------------------------|
| Vorhersagen | Regression | Klassifizierung |
| Verstehen/ErklÃ¤ren | Regression | Klassifizierung |
| Muster finden | Clustering | Clustering |
| Adaptiv handeln | Reinforcement Learning | Reinforcement Learning |

---

## 5. Der ML-Workflow

### 5.1 Die 3 Kernphasen

Jedes ML-Projekt folgt diesem Muster:

#### Phase 1: Representation (Darstellung)
**Entscheidungen:**
- Welche **Features** sollen wir verwenden?
  - Input: Rohbilder (Pixel)? Vorverarbeitete Features?
  - Welche 4 Messungen charakterisieren einen Apfel?
- Welchen **Klassifizierer-Typ** sollen wir wÃ¤hlen?
  - KNN? Decision Tree? Linear? Neural Network?

**Beispiel - Obstklassifizierung:**
- Features: height, width, mass, color_score
- Klassifizierer: k-Nearest Neighbors

#### Phase 2: Evaluation (Bewertung)
**Entscheidungen:**
- Welches **GÃ¼temaÃŸ** ist geeignet?
  - Accuracy? Precision? Recall? F1-Score?
- Wodurch unterscheiden sich gute vs. schlechte Klassifizierer?
  - Trainigsfehler vs. Testfehler
  - GeneralisierungsfÃ¤higkeit

#### Phase 3: Optimization (Optimierung)
**Entscheidungen:**
- Wie finde ich die **besten Parameter**?
  - Welcher k-Wert fÃ¼r KNN? (k=1, 3, 5, 10, â€¦?)
  - Hyperparameter Tuning

### 5.2 Der iterative Zyklus

```mermaid
graph TD
    A[" Start"] --> B[" Representation<br/>Features + Modell wÃ¤hlen"]
    B --> C[" Evaluation<br/>Modell bewerten"]
    C --> D[" Schlecht?"]
    D -->|"âœ… Ja"| B
    D -->|"âŒ Nein"| E[" Optimization<br/>Parameter tunen"]
    E --> F[" Verbesserung<br/>mÃ¶glich?"]
    F -->|"âœ… Ja"| E
    F -->|"âŒ Nein"| G[" Deployment<br/>Modell einsetzen"]

    B --> B1[Welche Features?]
    B --> B2[Welcher Algorithmus?]
    B --> B3[KNN, Linear Reg, etc.]

    C --> C1[Trainingsfehler]
    C --> C2[Testfehler]
    C --> C3[Overfitting checken]

    E --> E1[k-Wert bei KNN]
    E --> E2[Learning Rate]
    E --> E3[Hyperparameter]

    style G fill:#c8e6c9
```

```mermaid
flowchart TD
    A[" Representation<br/>Features+Modell"] --> B[" Evaluation<br/>Wie gut?"]

    B --> C[" Schlecht?"]
    C -->|"âœ… Ja"| A
    C -->|"âŒ Nein"| D[" Optimization<br/>Parameter tunen"]

    D --> E[" Verbesserung?"]
    E -->|"âœ… Ja"| D
    E -->|"âŒ Nein"| F[" Fertig!<br/>Deployment"]

    style F fill:#c8e6c9
```

```mermaid
flowchart TD
  D[" Dataset"] --> TR[" Train"]
  D --> TE[" Test (hold out)"]

  TR --> FIT[" Fit model on Train"]
  FIT --> ETR[" Evaluate on Train (optimistic)"]
  FIT --> ETE[" Evaluate on Test (realistic)"]

  ETR --> GAP[" Generalization gap = Train score - Test score"]
  ETE --> GAP

  GAP --> DIAG[" Gap large?"]
  DIAG -->|"âœ… Yes"| OF[" Overfitting: reduce complexity / add data / regularize"]
  DIAG -->|"âŒ No"| OK[" Good generalization"]
```

---

## 6. Daten: Kernkomponente jedes Modells

### 6.1 Der Datenfluss

**Universelle Wahrheit:** Kein Modell ohne Daten!

```
      Rohdaten
         â”‚
         â–¼
   Datenbereinigung
   (Fehler entfernen)
         â”‚
         â–¼
   Feature Engineering
   (Merkmale auswÃ¤hlen)
         â”‚
         â–¼
   Trainigsdaten
   (fÃ¼r das Modell)
         â”‚
         â–¼
   Trainiertes Modell
         â”‚
         â–¼
   Vorhersagen auf neuen Daten
```

### 6.2 DatenmodalitÃ¤ten

Die gleichen ML-Konzepte funktionieren auf verschiedenen **Datentypen**:

| Datentyp | Aufgaben |
|----------|----------|
| **Bilder** | Classification, Object Detection, Segmentation |
| **Tabellen (Zahlen)** | Regression, Classification, Clustering |
| **Text** | Classification, Language Translation, Text Generation |
| **Audio** | Speech-to-Text, Speaker Identification, Text-to-Speech |
| **Zeitserien** | Vorhersage (z.B. Aktienkurs) |

### 6.3 Kritische Anforderung: DatenÃ¤hnlichkeit

**Goldene Regel:**

> Trainingsdaten und zukÃ¼nftige Vorhersagedaten **mÃ¼ssen aus der gleichen DomÃ¤ne**
stammen!

**Gegenbeispiele (Funktioniert NICHT):**

| Training | Test | Problem |
|----------|------|---------|
| Bilder von SchweiÃŸnÃ¤hten (Industrie) | Bilder von Obst | VÃ¶llig andere DomÃ¤ne |
| SchwarzweiÃŸ-Handswritten Digits | Farbige Fotos von Ziffern | Unterschiedliche ModalitÃ¤t
|
| Text auf Englisch | Text auf Chinesisch | Verschiedene Sprachen |

**Problem mit zu wenig Daten:**

Wenn Sie nur wenige Daten haben:
1. **Typisch:** Kein Modell mÃ¶glich
2. **Ausnahme:** Transfer Learning
   - Ein vortrainiertes Modell auf Ã¤hnlichen Daten nutzen
- Beispiel: Modell trainiert auf 1 Million Dog-Bildern â†’ Anpassen auf 100
Bernhardiner-Bilder
   - **Wichtig:** Die Ã„hnlichkeit muss sehr hoch sein (z.B. auch Hunde)
   - Funktioniert **nicht** fÃ¼r: Finnisch nach Englisch, oder Radar-Bilder nach RGB-Bilder

---

## 7. Train/Validation/Test Split

### 7.1 Warum 3 Splits brauchen?

**Naives Vorgehen:** 1 Dataset â†’ Trainigsfehler messen
**Problem:** Wir **cheaten** unbewusst!

Wenn wir auf den gleichen Daten trainieren und testen, kommen wir zu **optimistischen
FehlerschÃ¤tzungen**.

**Konzept: Simulation von "unsichtbaren Daten"**

Wir teilen unser Dataset in 3 disjunkte Teile:

```mermaid
graph TD
A["  GESAMTES DATASET<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>100% der verfÃ¼gbaren Daten"] -->
TRAIN[" ğŸŸ¢ TRAINING SET (70%)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Modell lernt
Parameter<br/>Beispiele: Gewichte,
BÃ¤ume<br/>Use Case: Gradient Descent<br/>fit(X_train, y_train)"]

A --> VAL[" ğŸŸ¡ VALIDATION SET
(15%)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Hyperparameter-Optimierung<br/>Beispiele: Learning Rate,
k-NN k<br/>Use Case: Grid Search, Cross-Val<br/>Modell-Performance testen"]

A --> TEST["  TEST SET (15%)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Finale Evaluation<br/>NUR EINMAL
verwenden!<br/>Use Case: Unbiased SchÃ¤tzung<br/>score(X_test, y_test)"]

TRAIN --> MODEL["ğŸ¤– TRAINED MODEL<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Lernt aus Training
Data<br/>Optimiert Loss Function<br/>Bereit fÃ¼r Validation"]

VAL --> TUNING["âš™ï¸ HYPERPARAMETER TUNING<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Finde beste Settings:<br/>â€¢
Regularization Î»<br/>â€¢ Max Depth (Trees)<br/>â€¢ Hidden Layers (NN)"]

TEST --> FINAL[" FINAL METRICS<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Accuracy, F1-Score, AUC<br/>Echte
Generalisierung<br/>Nicht optimistisch!"]

    TUNING --> MODEL
    MODEL --> FINAL

A --> SPLIT[" WARUM SPLIT?<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Training: Parameter
lernen<br/>Validation: Settings optimieren<br/>Test: Unbiased Evaluation"]

    style TRAIN fill:#c8e6c9
    style VAL fill:#fff9c4
    style TEST fill:#ffccbc
    style MODEL fill:#b3e5fc
    style FINAL fill:#c8e6c9
```

    style D fill:#c8e6c9
```

```mermaid
flowchart LR
  RAW[" Raw data"] --> SPLIT[" Split FIRST"]
  SPLIT --> XTR[" X_train, y_train"]
  SPLIT --> XVA[" X_val, y_val"]
  SPLIT --> XTE[" X_test, y_test"]

  XTR --> SCFIT[" fit scaler / preprocess on X_train"]
  SCFIT --> TRS[" transform X_train"]
  SCFIT --> VAS[" transform X_val"]
  SCFIT --> TES[" transform X_test"]

  TRS --> MODEL[" fit model on transformed train"]
  MODEL --> EVA[" tune on val"]
  MODEL --> FINAL[" final score on test (once)"]

  BAD["Leakage (wrong): fit scaler on ALL data"] -.-> TES
  BAD -.-> FINAL
```

**Gesamtes Dataset**

| Dataset | Training (70%) | Validation (15%) | Test (15%) |
|---------|----------------|-----------------|------------|
| Zweck | Zum Trainieren des Modells | Zum Tunen der Hyperparameter | Zum Finalen Testen |

### 7.2 Zweck jeder Komponente

**Training Set (70%):**
- **Zweck:** Modellparameter lernen
- **Verwendung:** `model.fit(X_train, y_train)`
- **Nicht Ã¶ffentlich:** Modell sieht diese Daten

**Validation Set (15%):**
- **Zweck:** Hyperparameter tunen
- **Verwendung:** Test verschiedene k-Werte, Architekturen, etc.
- **Zweck:** Early Stopping erkennen
- **Problem:** Indirekt "Overfitting" auf Val-Set
- **LÃ¶sung:** Mit neuem Test-Set evaluieren

**Test Set (15%):**
- **Zweck:** **Finale unabhÃ¤ngige Evaluation**
- **Verwendung:** Erst am Ende! `model.score(X_test, y_test)`
- **Kritisch:** Niemals vor Finalem Report anschauen!
- **GÃ¼ltigkeit:** Erste (und einzige) ehrliche SchÃ¤tzung der Performance

### 7.3 Stratifiziertes Splitting

**Problem:** Bei unbalancierten Klassen kann zufÃ¤lliges Splitting zu Verzerrungen fÃ¼hren.

**Beispiel:**
- Gesamtdatensatz: 90% Apfel, 10% Banane
- ZufÃ¤lliger Split kÃ¶nnte fÃ¼hren zu:
  - Train: 85% Apfel, 15% Banane (nicht reprÃ¤sentativ!)
  - Test: 95% Apfel, 5% Banane

**LÃ¶sung:** Stratifiziertes Splitting
- ErhÃ¤lt die Klassenverteilung in jedem Split
- In scikit-learn: `stratify=y`

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    stratify=y,  # â† Wichtig bei Imbalance!
    random_state=42
)
```

---

## 8. K-Fold Cross-Validation

### 8.1 Das Problem mit festem Split

Bei kleinen DatensÃ¤tzen ist ein einzelner 70/30-Split **verschwendet**:
- 30% der Daten gehen nur fÃ¼r Test verloren
- Nur 70% zum Trainieren
- Statistik: Nur eine SchÃ¤tzung der Performance (mit hoher Varianz)

### 8.2 K-Fold Cross-Validation Prinzip

**Idee:** Jeden Datenpunkt **sowohl fÃ¼r Training als auch Validierung** nutzen!

#### Beispiel: 3-Fold Cross-Validation

```
**3-Fold Cross-Validation Beispiel:**

| Fold | Iteration 1 | Iteration 2 | Iteration 3 |
|------|-------------|-------------|-------------|
| Data | V, T, T | T, V, T | T, T, V |
| Validation | V | T | T |
| Training | T, T | V, T | T, V |
| Metric | 0.92 | 0.88 | 0.90 |

**Finale SchÃ¤tzung:**
- Durchschnitt = (0.92 + 0.88 + 0.90) / 3 = 0.90
- Std Dev = 0.017 (VariabilitÃ¤t)
```

### 8.3 Interpretation der Ergebnisse

**Report fÃ¼r K-Fold (k=10):**
```
KNeighborsClassifier: mean=0.96, std=0.023
SVC:                  mean=0.98, std=0.012
LogisticRegression:   mean=0.95, std=0.031
```

**Interpretation:**
- **Mean:** Durchschnittliche Performance Ã¼ber alle 10 Splits
- **Std:** Standard-Abweichung â†’ StabilitÃ¤t
  - Niedriges Std: Konsistent gutes Modell
  - Hohes Std: Modell ist volatil (manchmal gut, manchmal schlecht)

**Wichtig:** Cross-Validation **verbessert nicht** die Modellperformance!
- Sie gibt nur eine **bessere SchÃ¤tzung** der echten Performance
- Der gleiche Datensatz wird mehrfach recycliert

### 8.4 Wann Cross-Validation verwenden?

- âœ… Kleine DatensÃ¤tze (< 1000 Samples)
- âœ… Hyperparameter-Tuning
- âœ… Modellauswahl
- âŒ GroÃŸe DatensÃ¤tze (redundant und teuer)
- âŒ Online-Learning (kontinuierliches Training)

```mermaid
flowchart TD
    DATASET["GESAMTES DATASET<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Alle verfÃ¼gbaren Daten"]

DATASET --> OUTER[" OUTER
CROSS-VALIDATION<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Performance-SchÃ¤tzung<br/>5-fach CV"]

OUTER --> OTRAIN["ğŸŸ¢ OUTER TRAIN FOLDS (4/5)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>80% der Daten<br/>FÃ¼r
Hyperparameter-Tuning"]
OUTER --> OTEST[" OUTER TEST FOLD (1/5)<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>20% der
Daten<br/>UnberÃ¼hrte Performance-Test"]

OTRAIN --> INNER[" INNER CROSS-VALIDATION<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Auf
Outer-Train<br/>Hyperparameter optimieren"]

INNER --> HYPER[" HYPERPARAMETER TUNING<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Grid Search / Random
Search<br/>Finde beste k,
C, gamma, etc."]

HYPER --> BEST[" BESTE HYPERPARAMETER<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Aus Inner CV
gewÃ¤hlt<br/>Nicht Ã¼beroptimistisch"]

BEST --> REFIT[" REFIT AUF OUTER-TRAIN<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Gleiche
Hyperparameter<br/>Aber jetzt auf allen 80%<br/>des Outer-Trains"]

REFIT --> PREDICT[" PREDICT AUF OUTER-TEST<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>20% unberÃ¼hrte
Daten<br/>Echte Performance!"]

PREDICT --> SCORE[" SCORE BERECHNEN<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Accuracy, F1, AUC<br/>FÃ¼r
diesen Outer-Split"]

SCORE --> REPEAT[" WIEDERHOLE<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>FÃ¼r alle 5
Outer-Splits<br/>Verschiedene Test-Folds"]

REPEAT --> FINAL[" FINAL RESULT<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Durchschnitt aller 5
Scores<br/>Unbiased Performance-SchÃ¤tzung"]

    style DATASET fill:#fff9c4
    style OUTER fill:#c8e6c9
    style INNER fill:#b3e5fc
    style FINAL fill:#ffccbc
```

---

## 9. Datenexploration und Feature Engineering

### 9.1 Warum Daten visualisieren?

**Essenzielle GrÃ¼nde:**

1. **Fehler entdecken**
   - Falsche Datentypen
   - Fehlende Werte
   - AusreiÃŸer
   - Inkonsistenzen in Meseinheiten

2. **Klassenverteilung verstehen**
   - Sind Klassen balanciert?
   - Wie viele Samples pro Klasse?

3. **LÃ¶sbarkeit ohne ML checken**
   - "Kann ich das Problem ohne ML lÃ¶sen?"
   - Manchmal sind einfache Regeln ausreichend

4. **Features verstehen**
   - Welche Features diskriminieren gut zwischen Klassen?
   - Welche Features sind redundant?

### 9.2 Feature Representations

**Definition:** Ein **Feature** ist eine messbare GrÃ¶ÃŸe, die beschreibt, was wir
vorhersagen mÃ¶chten.

**Beispiel - Obstklassifizierung:**

| Objekt | Height | Width | Mass | Color Score | **Label** |
|--------|--------|-------|------|-------------|-----------|
| 1 | 7.0 | 8.0 | 155 | 0.67 | Apple |
| 2 | 4.5 | 3.2 | 40 | 0.75 | Mandarin |
| 3 | 9.2 | 8.5 | 190 | 0.52 | Orange |
| â€¦ | â€¦ | â€¦ | â€¦ | â€¦ | â€¦ |

**Features:** height, width, mass, color_score (Input Variables)
**Label:** Fruit Type (Target Variable)

**Mathematische Notation:**

```math

X = \begin{bmatrix}
7.0 & 8.0 & 155 & 0.67 \\
4.5 & 3.2 & 40 & 0.75 \\
9.2 & 8.5 & 190 & 0.52
\end{bmatrix}, \quad y = \begin{bmatrix} \mathrm{Apple} \\ \mathrm{Mandarin} \\
\mathrm{Orange} \end{bmatrix}

```

### 9.3 Feature-Typen

**Kontinuierliche Features:**
- Werte: Beliebige Zahlenwerte in einem Range
- Beispiele: height, weight, temperature

**Kategorische Features:**
- Werte: Festgelegte Kategorien
- Beispiele: Farbe (rot, grÃ¼n, blau), Stadt (ZÃ¼rich, Basel, Bern)
- Behandlung: **One-Hot-Encoding** fÃ¼r ML

**Ordinale Features:**
- Kategorisch aber mit **natÃ¼rlicher Ordnung**
- Beispiele: GrÃ¶ÃŸe (XS, S, M, L, XL), Sterne (â­ bis â­â­â­â­â­)

**BinÃ¤re Features:**
- Nur 2 Werte
- Beispiele: Ja/Nein, mÃ¤nnlich/weiblich, vorhanden/nicht-vorhanden

### 9.4 Feature Visualization Techniken

#### 2D Scatterplot
```python
plt.scatter(X['height'], X['width'], c=y, cmap='viridis')
plt.xlabel('Height')
plt.ylabel('Width')
plt.show()
```
**Erkenntnis:** Zeigt Cluster und Trennung zwischen Klassen in 2D

#### 3D Scatterplot
```python
fig.add_subplot(111, projection='3d')
ax.scatter3D(X['height'], X['width'], X['color_score'], c=y)
```
**Erkenntnis:** Addiert dritte Dimension, Klassengrenzen sichtbar?

#### Pairwise Feature Scatterplot (Seaborn)
```python
sns.pairplot(data, hue='label')
```
**Erkenntnis:** Alle **Kombinationen von Feature-Paaren**
- Diagonal: Histogramme einzelner Features
- Off-diagonal: Scatterplots von Feature-Paaren
- Farbe nach Label

#### Histogramme
```python
plt.hist(X[X['label']=='Apple']['height'], label='Apple', alpha=0.5)
plt.hist(X[X['label']=='Orange']['height'], label='Orange', alpha=0.5)
```
**Erkenntnis:** Verteilung eines Features pro Klasse

---

## 10. K-Nearest Neighbors (KNN) Klassifizierung

### 10.1 Grundkonzept

**Intuition:** "Der Apfel, der meinem Apfel am Ã¤hnlichsten sieht, ist wahrscheinlich auch
ein Apfel."

**Algorithmus:**

Gegeben:
- Trainigsdatensatz $X_{train}$ mit Labels $y_{train}$
- Ein neuer Punkt $x_{test}$ zum Klassifizieren
- Parameter $k$ (Anzahl der Nachbarn)

Vorgehen:
1. Berechne **AbstÃ¤nde** von $x_{test}$ zu **allen** $X_{train}$-Punkten
2. Finde die **k nÃ¤chsten** Punkte
3. Bestimme die **hÃ¤ufigste Klasse** unter diesen k Nachbarn
4. **Vorhersage** = diese hÃ¤ufigste Klasse

```mermaid
flowchart TD
START[" KNN ALGORITHMUS<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>K-Nearest Neighbors"] -->
INPUT["INPUT<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>x_test: neuer Punkt zu klassifizieren<br/>X_train:
Trainingsdaten<br/>y_train: Labels<br/>k: Anzahl Nachbarn"]

INPUT --> DIST[" DISTANCE COMPUTATION<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Berechne Distanz von
x_test<br/>zu ALLEN Trainingspunkten<br/>Euklidisch,
Manhattan, etc."]

DIST --> SORT[" SORT BY DISTANCE<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Ordne alle Punkte
nach<br/>aufsteigender Distanz zu x_test"]

SORT --> SELECT[" SELECT K NEAREST<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Nimm die ersten k
Punkte<br/>z.B. k=3,
k=5, k=7"]

SELECT --> VOTE[" VOTE FOR CLASS<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Majority Vote: HÃ¤ufigste
Klasse<br/>Distance-weighted: Nahe Punkte stÃ¤rker gewichtet"]

VOTE --> PRED[" PREDICTION<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Gewinner-Klasse = Vorhersage<br/>FÃ¼r
x_test"]

SELECT --> EXAMPLE[" BEISPIEL<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>k=3 Nachbarn:<br/>â€¢ 2Ã— 'Apfel'
(nah)<br/>â€¢ 1Ã— 'Banane' (weit)<br/>â†’ Vorhersage: 'Apfel'"]

    style START fill:#fff9c4
    style DIST fill:#c8e6c9
    style SELECT fill:#b3e5fc
    style PRED fill:#ffccbc
    style EXAMPLE fill:#ffe0b2
```

### 10.2 Distanzmetriken

**Euklidische Distanz** (Standard):

```math

d(x_i, x_j) = \sqrt{\sum_{m=1}^{p} (x_{i,m} - x_{j,m})^2}

```

**Beispiel:** Zwei ObststÃ¼cke mit Features (height, width):
- Punkt A: (7.0, 8.0)
- Punkt B: (6.5, 8.2)

```math

d(A, B) = \sqrt{(7.0 - 6.5)^2 + (8.0 - 8.2)^2} = \sqrt{0.25 + 0.04} = \sqrt{0.29} \approx
0.54

```

**Beispiel:** Zwei ObststÃ¼cke mit Features (height, width):
- Punkt A: (7.0, 8.0)
- Punkt B: (6.5, 8.2)

```math

d(A, B) = \sqrt{(7.0 - 6.5)^2 + (8.0 - 8.2)^2} = \sqrt{0.25 + 0.04} = \sqrt{0.29} \approx
0.54

```

**Andere Distanzen:**
- **Manhattan-Distanz:** $|x_{i,1} - x_{j,1}| + |x_{i,2} - x_{j,2}| + \ldots$
- **Chebyshev-Distanz:** $\max(|x_{i,1} - x_{j,1}|, |x_{i,2} - x_{j,2}|, \ldots)$

### 10.3 Parameter und Konfiguration

KNN benÃ¶tigt 3+ Entscheidungen:

**1. Distanzmetrik** (typisch: Euklidisch)
```python
knn = KNeighborsClassifier(metric='euclidean')
```

**2. k-Wert** (wie viele Nachbarn?)
```python
knn = KNeighborsClassifier(n_neighbors=5)
```
- k=1: **Zu flexibel** (memoriert Trainigsdaten)
- k=5-10: **Gutes MittelmaÃŸ** (unsere Wahl)
- k=100: **Zu starr** (glÃ¤ttet zu sehr)

**3. Aggregationsmethode** (wie kombiniere ich k Nachbarklassen?)
- **Majority Voting** (Standard): HÃ¤ufigste Klasse
- **Weighted Voting:** NÃ¤here Nachbarn zÃ¤hlen mehr

**4. Optional: Gewichtung**
```python
knn = KNeighborsClassifier(
    n_neighbors=5,
    weights='distance'  # NÃ¤here Punkte zÃ¤hlen mehr
)
```

### 10.4 Visualisierung: Entscheidungsgrenzen

Mit k=1:
- Jede Region gehÃ¶rt zum nÃ¤chsten Trainigspunkt
- **Sehr wackelige** Grenzen (Memorieren)

Mit k=5:
- Glattere Grenzen
- Besser generalisierbar

Mit k=45:
- **Sehr glatte** Grenzen
- Aber: Underfitting mÃ¶glich

**Entscheidungsgrenzen: k-Wert Effekt**

```mermaid
xychart-beta
    title "KNN Entscheidungsgrenzen: k-Wert Effekt"
    x-axis [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    y-axis 0 --> 10
    line [2, 3, 4, 5, 6, 7, 8, 7.5, 7, 6.5]
    line [3, 4, 5, 6, 7, 6.8, 6.5, 6.3, 6, 5.8]
```

**Legende:**
- Blaue Linie: k=1 (komplexe, wackelige Grenzen)
- Orange Linie: k=15 (glattere, stabilere Grenzen)

**k=1:** Sehr flexible, komplexe Grenzen (hohe Variance)
**k=15:** Stabilere, einfachere Grenzen (hÃ¶herer Bias)

### 10.5 Vor- und Nachteile KNN

| Vorteil | Nachteil |
|---------|----------|
| âœ… Sehr einfach zu verstehen | âŒ Abstandsberechnung teuer fÃ¼r groÃŸe DatensÃ¤tze |
| âœ… Keine Trainingsphase nÃ¶tig | âŒ Schlecht bei hochdimensionalen Daten (Curse of
Dimensionality) |
| âœ… Funktioniert mit kleinen Datasets | âŒ Sensitive gegenÃ¼ber Datenskalierung |
| âœ… Ergebnisse interpretierbar | âŒ Alle Features mÃ¼ssen numerisch sein |

```mermaid
graph TD
A["  CURSE OF DIMENSIONALITY<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Fluch der hohen
Dimensionen<br/>'VerdÃ¼nnung' der Daten"] --> DIM2["  2D
RAUM<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Punkte bilden Cluster<br/>Lokale Strukturen sichtbar"]

A --> DIM10["  10D RAUM<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Volumen wÃ¤chst exponentiell<br/>Daten
werden 'verdÃ¼nnt'<br/>Alle Punkte liegen am Rand"]

DIM2 --> DIST2[" DISTANZEN in 2D<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Nearest Neighbor: nah<br/>Farthest
Neighbor: weit<br/>Klares VerhÃ¤ltnis"]

DIM10 --> DIST10[" DISTANZEN in 10D<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Nearest â‰ˆ Farthest<br/>Alle
Distanzen Ã¤hnlich<br/>Schwierig zu unterscheiden"]

DIST2 --> KNN_GOOD["âœ… KNN in 2D<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Funktioniert gut<br/>Lokale Nachbarn
relevant"]

DIST10 --> KNN_BAD["âŒ KNN in 10D<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Performance sinkt<br/>Alle Punkte
gleich weit<br/>ZufÃ¤llige Vorhersagen"]

DIM10 --> GENERAL[" ALLGEMEINES PROBLEM<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>In hoher Dimension:<br/>â€¢
Distanzen konzentrieren sich<br/>â€¢ Lokale Struktur geht verloren<br/>â€¢ Overfitting
wahrscheinlicher"]

    style A fill:#fff9c4
    style DIM2 fill:#c8e6c9
    style DIM10 fill:#ffccbc
    style KNN_BAD fill:#ffb3ba
    style GENERAL fill:#ffe0b2
```

---

## 11. Daten-Skalierung

### 11.1 Das Skalierungsproblem

**Problem:** Features haben unterschiedliche **Skalen**!

**Beispiel:**
- Height: 3-10 cm
- Weight: 40-500 g

Euklidische Distanz:

```math

d = \sqrt{(h_1 - h_2)^2 + (w_1 - w_2)^2}

```

**Problem:** Weight-Unterschiede dominieren! $(400)^2 >> (7)^2$

**Visualization:**

```mermaid
xychart-beta
    title "Unscaled Data: Weight dominates distance calculation"
    x-axis "Height (cm)" 3 --> 7
    y-axis "Weight (g)" 0 --> 500
    line [4.2, 4.2, 7, 4.2]
    scatter [3.5, 4.8, 5.2, 6.1] [50, 200, 350, 450]
```

```mermaid
xychart-beta
    title "Scaled Data: Balanced feature importance"
    x-axis "Height (scaled)" 0 --> 1
    y-axis "Weight (scaled)" 0 --> 1
    line [0.4, 0.6, 0.6, 0.4]
    scatter [0.2, 0.6, 0.8, 0.95] [0.1, 0.4, 0.7, 0.9]
```

**Entscheidungsgrenzen:**
- **Links (unskaliert):** Nicht-sphÃ¤risch, verzerrt durch Weight-Dominanz
- **Rechts (skaliert):** SchÃ¶n sphÃ¤risch, beide Features gleich wichtig

```mermaid
graph TD
PROBLEM[" SCALING PROBLEM<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Features haben unterschiedliche
Skalen<br/>â†’ Ein Feature dominiert Distanzberechnungen"]

PROBLEM --> UNSCALED["âŒ OHNE SKALIERUNG<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Height: 3-10 cm<br/>Weight:
40-500 g<br/>â†’ Weight dominiert!"]

    UNSCALED --> EXAMPLE_U[" BEISPIEL<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Punkt A: Height=5,
Weight=100<br/>Punkt B: Height=8, Weight=200<br/>Distanz â‰ˆ 100 (nur Weight!)"]

PROBLEM --> SCALED["âœ… MIT SKALIERUNG<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Height: 0-1
(Min-Max)<br/>Weight: 0-1 (Min-Max)<br/>â†’ Beide Features gleich wichtig"]

    SCALED --> EXAMPLE_S[" BEISPIEL<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Punkt A: Height=0.25,
Weight=0.15<br/>Punkt B: Height=0.63, Weight=0.32<br/>Distanz berÃ¼cksichtigt beide!"]

UNSCALED --> KNN_U["ğŸ¤– KNN RESULT<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Nachbarn nur nach Weight<br/>Height
ignoriert<br/>Schlechte Klassifikation"]
SCALED --> KNN_S["ğŸ¤– KNN RESULT<br/>â”â”â”â”â”â”â”â”â”â”â”â”â”<br/>Ausgewogene Nachbarn<br/>Alle
Features relevant<br/>Bessere Klassifikation"]

    style PROBLEM fill:#fff9c4
    style UNSCALED fill:#ffb3ba
    style SCALED fill:#c8e6c9
    style KNN_S fill:#b3e5fc
```

### 11.2 Skalierungsmethoden

#### MinMaxScaler (Normalisierung)

Transformiere jeden Feature auf $[0, 1]$:

```math

x'_i = \frac{x_i - \min(x)}{\max(x) - \min(x)}

```

**Beispiel:**
- Original: height = 7.5, min=3, max=10
- Skaliert: $\frac{7.5 - 3}{10 - 3} = \frac{4.5}{7} \approx 0.64$

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_train)  # Nur Training fitten!
X_test_scaled = scaler.transform(X_test)  # Mit Train-Parametern!
```

#### StandardScaler (Standardisierung)

Transformiere zu **Mittelwert = 0, Std-Dev = 1**:

```math

x'_i = \frac{x_i - \mu}{\sigma}

```

Wobei:
- $\mu$ = Mittelwert
- $\sigma$ = Standardabweichung

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
```

### 11.3 Kritische Regeln

**MUSS beachtet werden:**

1. **Nur auf Trainigsdaten fitten!**
   ```python
   scaler.fit(X_train)  # âœ… Richtig
   scaler.fit(X_test)   # âŒ Falsch!
   ```

2. **Gleichen Scaler auf Test anwenden!**
   ```python
   X_train_scaled = scaler.fit_transform(X_train)
   X_test_scaled = scaler.transform(X_test)  # Mit Train-Parametern
   ```

3. **Warum?** Wenn wir Test-Statistiken nutzen, "leaken" wir Information aus Test in
Training!

---

## 12. Lineare Regression

### 12.1 Regression vs. Klassifizierung

| Aspekt | Klassifizierung | Regression |
|--------|-----------------|-----------|
| Output | Diskrete Klasse | Kontinuierliche Zahl |
| Beispiele | Spam/Nicht-Spam | Preis, Temperatur |
| Ausgabefunktion | Diskontinuierlich | Kontinuierlich |

### 12.2 Lineares Regressionsmodell

**Definition:** Finde Parameter $\beta_1, \beta_0$ so dass:

```math

\hat{y} = \beta_1 \cdot x + \beta_0

```

Wobei:
- $x$ = Input
- $\hat{y}$ = Vorhersage
- $\beta_1$ = Steigung (Coefficient)
- $\beta_0$ = Intercept (y-Achsen-Abschnitt)

**Ziel:** Minimiere Fehler zwischen $y$ (RealitÃ¤t) und $\hat{y}$ (Vorhersage)

```mermaid
flowchart LR
  PTS["Data points (x, y)"] --> LINE["Candidate line Å· = Î²1 x + Î²0"]
  LINE --> RES["Residuals: (y - Å·) per point"]
  RES --> MSE["MSE = average of squared residuals"]
  MSE --> OPT["Choose Î² to minimize MSE"]
```

### Gradient Descent: Wie Parameter optimiert werden

```mermaid
graph TD
    A[Start mit zufÃ¤lligen Parametern] --> B[Berechne Vorhersage Å·]
    B --> C[Berechne Fehler/Verlust L]
    C --> D[Berechne Gradient âˆ‚L/âˆ‚Î¸]
    D --> E[Update Parameter: Î¸ = Î¸ - Î± * âˆ‡L]
    E --> F{Ist Minimum<br/>erreicht?}
    F -->|Nein| B
    F -->|Ja| G[Optimale Parameter gefunden]

    H[Beispiel: Lineare Regression] --> H1[L = Î£(y - Å·)Â²]
    H --> H2[âˆ‡L = âˆ‚L/âˆ‚m, âˆ‚L/âˆ‚b]
    H --> H3[Update: m,b mit Lernrate Î±]

    I[Hyperparameter] --> I1[Lernrate Î±: Zu klein=slow, Zu groÃŸ=overshoot]
    I --> I2[Batch Size: Full vs Mini vs Stochastic]
    I --> I3[Epochs: Wie oft durch Daten gehen]

    style A fill:#ffebee
    style G fill:#c8e6c9
```

```mermaid
flowchart TD
    START[" START Gradient Descent<br/>Initialisiere Î¸ = [0.5,
1.2]<br/>Learning Rate Î± = 0.01"] --> ITER1[" ITERATION 1<br/>â”â”â”â”â”â”â”<br/>Î¸ = [0.5,
1.2]<br/>Loss L(Î¸) = 45.2<br/>âˆ‡L(Î¸) = [12.3, -8.7]"]

ITER1 --> UPDATE1[" UPDATE Î¸<br/>â”â”â”â”â”â”â”<br/>Î¸ â† Î¸ - Î±âˆ‡L(Î¸)<br/>Î¸ = [0.5, 1.2] -
0.01Ã—[12.3,
-8.7]<br/>Î¸ = [0.377, 1.287]"]

    UPDATE1 --> ITER2[" ITERATION 2<br/>â”â”â”â”â”â”â”<br/>Î¸ = [0.377,
1.287]<br/>Loss L(Î¸) = 23.8<br/>âˆ‡L(Î¸) = [6.1, -4.2]"]

    ITER2 --> UPDATE2[" UPDATE Î¸<br/>â”â”â”â”â”â”â”<br/>Î¸ â† Î¸ - Î±âˆ‡L(Î¸)<br/>Î¸ = [0.377,
1.287] - 0.01Ã—[6.1, -4.2]<br/>Î¸ = [0.316, 1.329]"]

    UPDATE2 --> ITER3[" ITERATION 3<br/>â”â”â”â”â”â”â”<br/>Î¸ = [0.316,
1.329]<br/>Loss L(Î¸) = 12.4<br/>âˆ‡L(Î¸) = [3.2, -2.1]"]

    ITER3 --> CONVERGE{CONVERGENCE?<br/>â”â”â”â”â”â”â”<br/>Loss < 0.1<br/>OR<br/>Gradient â‰ˆ 0}

    CONVERGE -->|Nein| UPDATE3[" UPDATE Î¸<br/>Î¸ = [0.316, 1.329] - 0.01Ã—[3.2,
-2.1]<br/>Î¸ = [0.284, 1.350]"]
    UPDATE3 --> ITER4[" ITERATION 4..."]

CONVERGE -->|Ja| FINAL["âœ… FINAL Î¸<br/>â”â”â”â”â”â”â”<br/>Î¸ â‰ˆ [0.33, 1.8]<br/>Loss â‰ˆ
0.05<br/>Konvergiert!"]

    ITER4 --> CONVERGE

    style START fill:#c8e6c9
    style ITER1 fill:#b3e5fc
    style UPDATE1 fill:#81d4fa
    style ITER2 fill:#ffe0b2
    style UPDATE2 fill:#ffccbc
    style ITER3 fill:#ffb3ba
    style FINAL fill:#c8e6c9
```

#### Visualisierung: Gradient Descent in Aktion

```mermaid
flowchart TD
    subgraph "Loss Function L(Î¸)"
        A["Loss Value"] --> B["5"]
        B --> C["3"]
        C --> D["1"]
        D --> E["0"]

        F["Î¸ (Parameter)"] --> G["Î¸â‚"]
        G --> H["Î¸â‚ƒ"]

I["â— Ziel-Minimum<br/>Gradient Descent<br/>bewegt sich den<br/>steilsten
Abhang<br/>hinunter!"]
    end

    style I fill:#c8e6c9
```

### 12.2.1 Learning Curve: Training-Progress visualisiert

**Was passiert wÃ¤hrend des Trainings?**
- **Loss:** Wie gut passt das Modell zu den Daten?
- **Training Loss:** Wie gut auf Trainingsdaten?
- **Validation Loss:** Wie gut auf ungesehenen Daten?

```mermaid
xychart-beta
    title "Learning Curve: Loss Ã¼ber Training-Epochen"
    x-axis [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
    y-axis 0 --> 100
    line [90, 65, 45, 30, 20, 15, 12, 10, 8, 6, 5]
    line [95, 70, 55, 45, 40, 38, 37, 36, 35, 35, 35]
```

**Legende:**
- Blaue Linie: Training Loss (fÃ¤llt kontinuierlich)
- Orange Linie: Validation Loss (fÃ¤llt dann stagniert/steigt)

**Interpretation:**
- **Training Loss â†“:** Modell lernt die Trainingsdaten
- **Validation Loss â†“ dann â†’:** Anfangs Generalisierung, dann Overfitting
- **Optimal:** Stoppe bei Minimum der Validation Loss
- **Overfitting:** Wenn Validation Loss wieder steigt

### 12.3 Beispiel: Fahrenheit zu Celsius

**Problem:** Konvertiere Temperatur von Â°F zu Â°C

**Wahre Funktion** (mÃ¼ssen wir nicht kennen, ist das Ziel!):
```math

y = (x - 32) \cdot \frac{5}{9}

```

**Trainigsdaten (mit Messfehler):**

| Â°F | Â°C |
|----|-----|
| 50 | 10.0 |
| 86 | 30.2 |
| 104 | 40.1 |
| 113 | 45.0 |
| 122 | 50.2 |

**Lineares Modell finden:**

LÃ¶se das Gleichungssystem:
```math

\begin{align}
10.0 &= 50 \cdot \beta_1 + \beta_0 \\
30.2 &= 86 \cdot \beta_1 + \beta_0 \\
40.1 &= 104 \cdot \beta_1 + \beta_0 \\
45.0 &= 113 \cdot \beta_1 + \beta_0 \\
50.2 &= 122 \cdot \beta_1 + \beta_0
\end{align}

```

**Ergebnis:** [ \beta_1 \approx 0.556, \beta_0 \approx -17.78 ]

**Modell:** $\hat{y} = 0.556 \cdot x - 17.78$

### 12.4 Lineare Regression in scikit-learn

```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train.reshape(-1, 1), y_train)

# Vorhersagen
y_pred = lr.predict(X_test.reshape(-1, 1))

# Parameter
print(f"Slope: {lr.coef_[0]:.4f}")
print(f"Intercept: {lr.intercept_:.4f}")
```

### 12.5 Bewertungsmetriken fÃ¼r Regression

**Mean Squared Error (MSE):**
```math

MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2

```
- Fehler werden quadriert â†’ groÃŸe Fehler stark bestraft
- Intuition: Durchschnittliches quadriertes Fehler

**Mean Absolute Error (MAE):**
```math

MAE = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|

```
- Intuitive Interpretation
- Weniger sensitive gegenÃ¼ber AusreiÃŸern

---

## 13. Logistische Regression und Entscheidungsgrenzen

### 13.1 Das Problem mit linearer Regression fÃ¼r Klassifizierung

**Naiv:** Nutze lineare Regression fÃ¼r Klassifizierung

**Modell:** $\hat{y} = \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_0$

**Problem:** Output ist **unbegrenzt**!

Beispiel: Modell gibt $\hat{y} = -5$ oder $\hat{y} = 100$ aus
- Aber wir brauchen Wahrscheinlichkeit zwischen 0 und 1!

### 13.2 Sigmoid-Funktion

**LÃ¶sung:** Quetsche linearen Output durch **Sigmoid-Funktion**:

```math

\sigma(z) = \frac{1}{1 + e^{-z}}

```

**Eigenschaften:**
- Input: beliebig ($-\infty$ bis $+\infty$)
- Output: immer zwischen 0 und 1
- S-fÃ¶rmig (daher "Sigmoid")
- Differenzierbar (wichtig fÃ¼r Optimierung)

**Visualisierung:**

```mermaid
xychart-beta
    title "Kumulative Normalverteilung Ïƒ(z)"
    x-axis -5 --> 5
    y-axis 0 --> 1
    line [0, 0.006, 0.023, 0.159, 0.5, 0.841, 0.977, 0.994, 1]
```

Ïƒ(-âˆ) = 0,  Ïƒ(0) = 0.5,  Ïƒ(+âˆ) = 1

### 13.3 Logistische Regression

**Modell:**

Erst: Lineare Funktion
```math
z = \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_0
```

Dann: Durch Sigmoid
```math
\hat{p} = \sigma(z) = \frac{1}{1 + e^{-z}} \in [0, 1]
```

```mermaid
flowchart LR
  X["Features (x1, x2, ...)"] --> Z["Linear score z = Î²Â·x + Î²0"]
  Z --> S["Sigmoid: p = 1/(1+e^{-z})"]
  S --> T{p > 0.5?}
  T -->|Yes| C1["Predict class 1"]
  T -->|No| C0["Predict class 0"]
```

**Interpretation:** [ \hat{p} ] = Wahrscheinlichkeit fÃ¼r Klasse 1

**Entscheidungsregel:**
- Wenn [ \hat{p} > 0.5 ]: Vorhersage = Klasse 1
- Wenn [ \hat{p} \leq 0.5 ]: Vorhersage = Klasse 0

### 13.4 Entscheidungsgrenzen mit linearem Modell

**Entscheidungsgrenze:** Die Kurve, wo [ \hat{p} = 0.5 ]

Gegeben 2 Features [ x_1, x_2 ]:
[
0.5 = \sigma(\beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_0)
]

Inverse Sigmoid (Logit):
[
\beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_0 = 0
]

**Das ist eine Linie!** (oder Hyperplane in hÃ¶heren Dimensionen)

**Beispiel:**
- Parameter gefunden: [ \beta_1 = 1, \beta_2 = 1.5, \beta_0 = 0 ]
- Entscheidungsgrenze: [ 1 \cdot \text{mass} + 1.5 \cdot \text{height} = 0 ]

```mermaid
flowchart TD
  Z0["Decision boundary: z = Î²Â·x + Î²0 = 0"] --> LIN["In 2D: a line"]
  LIN --> HP["In higher-D: a hyperplane"]
  HP --> LIM["Limitation: only linear separations without feature transforms"]
```

### 13.5 Limitation: Lineare Grenzen

**Problem:** Logistische Regression kann **nur lineare Grenzen** zeichnen!

Wenn Daten nicht **linear separabel** sind, funktioniert's nicht gut.

**LÃ¶sung:** Komplexere Modelle
- Decision Trees (beliebig geformte Grenzen)
- Support Vector Machines (mit Kernel)
- Neural Networks (beliebig komplexe Grenzen)

---

## ğŸ“š Zusammenfassung der Kernkonzepte

| Konzept | Definition | Beispiel |
|---------|-----------|----------|
| **Feature** | Messbare EingabegrÃ¶ÃŸe | height=7.5cm |
| **Label/Target** | Zu vorhersagende GrÃ¶ÃŸe | fruit_type=Apple |
| **Training** | Modell mit Daten fittenlernen | `model.fit(X, y)` |
| **Inference** | Vorhersage machen | `model.predict(X_new)` |
| **Generalisierung** | Gut auf **neuen** Daten arbeiten | Train â‰  Test |
| **KNN** | k-Nearest Neighbors Klassifizierer | k=5, Mehrheitsvoting |
| **Regression** | Vorhersage kontinuierlicher Wert | Temperaturvorhersage |
| **Klassifizierung** | Vorhersage diskrete Klasse | Apfel vs. Orange |
| **Cross-Validation** | k-Fold Datenzirkulation | k=10, bessere SchÃ¤tzung |
| **Skalierung** | Daten normalisieren/standardisieren | MinMaxScaler, StandardScaler |

---

## ğŸ¯ Lernziele

Nach diesem Skript sollten Sie verstehen:

1. âœ… **Motivationen:** Warum ML nutzen? Wann ist es sinnvoll?
2. âœ… **Grundkonzepte:** Supervised vs. Unsupervised, Classification vs. Regression
3. âœ… **Historischer Kontext:** AI Winter, Perceptron, Deep Learning Revolution
4. âœ… **Workflow:** Representation â†’ Evaluation â†’ Optimization
5. âœ… **Daten-Paradigma:** Kein Modell ohne Daten, Modality matching
6. âœ… **Train/Val/Test:** Warum 3 Splits? Generalisierung
7. âœ… **Cross-Validation:** k-Fold fÃ¼r kleine DatensÃ¤tze
8. âœ… **Datenexploration:** Features verstehen, Visualisierung
9. âœ… **KNN-Algorithmus:** Intuition, Parameter, Entscheidungsgrenzen
10. âœ… **Skalierung:** Warum? MinMax vs. Standard Scaler
11. âœ… **Lineare Regression:** Parameter finden, MSE/MAE
12. âœ… **Logistische Regression:** Sigmoid-Funktion, lineare Grenzen
13. âœ… **Praktische Implementierung:** scikit-learn Code fÃ¼r alle Konzepte

---

## ğŸ“– WeiterfÃ¼hrende Ressourcen

- **scikit-learn Dokumentation:** https://scikit-learn.org/
- **Hands-On ML Book:** "Hands-On Machine Learning" (AurÃ©lien GÃ©ron)
- **Feature Scaling:** https://scikit-learn.org/stable/modules/preprocessing.html
- **Cross-Validation:** https://scikit-learn.org/stable/modules/cross_validation.html
- **ML Map:** https://scikit-learn.org/stable/tutorial/machine_learning_map/

---

*Skript erstellt: Dezember 2025 - Basierend auf FCS-BWL Machine Learning 1 Kurs (Woche
10)*